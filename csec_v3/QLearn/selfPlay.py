from hearts.arena import FourPlayerArena
from hearts.game import GameState
from QLearn.network_translate import select_action_decorator
from QLearn.architectures import make_network_copy
from QLearn.train import QTrainer
from QLearn.reward import get_reward

from tensorflow import keras
import time

class SelfPlay:
    """
    This function will run a self-play algorithm on the Neural Network generated by constructor, saving the results at checkpoint_path.
    """
    def __init__(self, constructor, starting_state = GameState, num_iterations = 52*64, evaluation_cutoff_PPG = 0.5, network_translator = select_action_decorator,
            checkpoint_path = None, use_arena_data = True, reset_history = False, DQN_Flag = True, gamma = 0.99,
            state_dim = 472, action_dim = 52, verbose = False):
        """
        This function will run a self-play algorithm on the Neural Network generated by constructor, saving the results at checkpoint_path.
        If verbose, this will print time outputs.

        Required
        constructor = This is the function that is used to create the neural network

        Important
        network_translator = How the Self-Play module will evaluate Neural Networks. This is a decorator that takes a Neural Network and returns a player object that can play based on the Neural Network. The default is select_action_decorator, which just plays the highest q-value.
        checkpoint_path = Directory to save Neural Network Ouputs. This will save outputs in subfolders (1), (2), and so on, based on the number of training_steps taken.
        use_arena_data = If this is True, after iteration (1) this will use data from Arena gameplay to train the network.
        num_iterations = Total amount of states of data that will be evaluated. Arena will evaluate in "num_iterations/52" games. training will occur in num_iterations instances.

        Not Important
        reset_history = If this is True, this will reset the gameplay cache every iteration.
        evaluation_cutoff_PPG = If a Neural network is this many points per game better than an old neural network, the network is replaced. Set to 0 by default, but can be set to something like 1 if you want to be ju
        DQN_Flag = If this is True, we will evaluate future Q-learning samples against a Target network for all non-bootstrap phases
        Gamma = This determines the Gamma used in our Q-values

        No Adjustment Needed
        state_dim = Input Size for Neural Network
        action_dim = Output Size for Neural Network
        starting_state = Game Initalizer
        """
        self.constructor = constructor
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.checkpoint_path = checkpoint_path
        self.verbose = verbose
        self.num_iterations = num_iterations
        self.network_translator  = network_translator
        self.evaluation_iterations = int(num_iterations / 52)
        self.evaluation_cutoff_PPG = evaluation_cutoff_PPG
        self.use_arena_data = use_arena_data
        self.reset_history = reset_history
        self.DQN_Flag = DQN_Flag

        self.qt = QTrainer(get_reward, starting_state, gamma = gamma)
        self.arena = FourPlayerArena(starting_state, default_num_iterations=self.evaluation_iterations, keep_history = self.use_arena_data)

        if self.verbose:
            self.start_time = time.time()

    def save(self):
        self.q_network.compile(optimizer="adam", loss="huber")
        self.q_network.save(self.checkpoint_path + str(self.training_steps))
        if self.verbose:
            print("Saved Model at " + self.checkpoint_path + str(self.training_steps))

    def train(self, opponent_agent = None):
        if self.verbose:
            t = time.time()

        if self.checkpoint_path:
            self.save()
        
        old_q_network = make_network_copy(self.q_network, self.state_dim, self.action_dim, self.constructor)
        if not opponent_agent:
            OA = self.network_translator(old_q_network)
        else:
            OA = opponent_agent

        if self.DQN_Flag:
            self.qt.train_q_network(self.q_network, self.num_iterations, target_q_network = old_q_network, verbose = self.verbose, opponent_agent = OA, add_new_episodes = not self.use_arena_data)
        else:
            self.qt.train_q_network(self.q_network, self.num_iterations, target_q_network = None, verbose = self.verbose, opponent_agent = OA, add_new_episodes = not self.use_arena_data)

        if self.verbose:
            print("Model Training Time: " + str(time.time() - t))
            t = time.time()
        
        eval_result = self.arena.winning_points(self.network_translator(self.q_network), OA, num_iterations = self.evaluation_iterations)
        if self.use_arena_data:
            eval_result, new_history = eval_result
            self.qt.load_history(new_history)
        if self.verbose:
            print(eval_result)
        
        if eval_result > - self.evaluation_iterations * self.evaluation_cutoff_PPG and opponent_agent is None:
            if not opponent_agent:
                if self.verbose:
                    print("Iteration " + str(self.training_steps) + " Newly Trained Network is not better")
                self.q_network = make_network_copy(old_q_network, self.state_dim, self.action_dim, self.constructor)
            if self.verbose:
                print("Iteration " + str(self.training_steps) + " Continuing Initial Training")
        else:
            if self.verbose:
                print("Iteration " + str(self.training_steps) + " Replace with New Network")

        if self.verbose:
            print("Model Selection Time: " + str(time.time() - t))
            t = time.time()
    
    def iterate(self, training_limit = None, opponent_agent = None):


        if self.use_arena_data and opponent_agent is None: #if we use arena data, we will want to generate a fresh round of data first.
            if opponent_agent:
                eval_result = self.arena.winning_points(self.network_translator(self.q_network), opponent_agent, num_iterations = self.evaluation_iterations)
            else:
                eval_result = self.arena.winning_points(self.network_translator(self.q_network), self.network_translator(self.q_network), num_iterations = self.evaluation_iterations)
            eval_result, new_history = eval_result
            self.qt.load_history(new_history)
            if self.verbose:
                print(eval_result)
                print("Regular Training Starts Now " + str(time.time() - self.start_time))
        
        while (not training_limit) or (self.training_steps < training_limit):
            self.training_steps += 1
            self.train(opponent_agent = opponent_agent)
            if self.reset_history:
                self.qt.reset_history()
            if self.verbose:
                print(str(self.training_steps) + " Training Done: " + str(time.time() - self.start_time) + "\n")
        
    def __call__(self, bootstrap_opponent = None, starting_agent = None, starting_agent_number = None, training_limit = None, extended_bootstrap = False):
        """
        If starting agent and bootstrap_opponent are None, the training will start using random weights. This is highly unstable. [Not Recommended]
        If starting agent is None and bootstrap_opponent exists, the training will start using a blank network, using the opponent_agent as a bootstrap.  [Recommended]
        If starting agent exists, the training will start using an existing network, playing against itself. [Recommended]
        """

        if not starting_agent:
            self.training_steps = 0
            self.q_network = self.constructor(self.state_dim, self.action_dim)
            if self.checkpoint_path:
                self.save()
            if bootstrap_opponent:
                self.qt.train_q_network(self.q_network, self.num_iterations, verbose = self.verbose, opponent_agent = bootstrap_opponent)
                if self.reset_history:
                    self.qt.reset_history()
                if extended_bootstrap:
                    self.iterate(training_limit, opponent_agent = bootstrap_opponent)
            if self.verbose:
                print("Initial Training Done: " + str(time.time() - self.start_time))
            
        else:
            if not starting_agent_number:
                starting_agent_number = int(starting_agent.split("/")[-1])
            self.training_steps = starting_agent_number
            self.q_network = keras.models.load_model(starting_agent)

        self.iterate(training_limit)

